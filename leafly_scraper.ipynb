{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Cell \n",
    "\n",
    "Everything that works is addded to the cell below, you can complete 100% of the scraping by running it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#started at 1:19 pm, 4/10\n",
    "#started at 3:06 pm, 4/26\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "#start by getting the strain names/links\n",
    "URL = 'https://www.leafly.com/strains?sort=name'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "href = []\n",
    "for x in range(1, 115):\n",
    "    #loop through all strain webpages\n",
    "    URL = 'https://www.leafly.com/strains?sort=name&page='+ str(x)\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    #get arrays containing strain info\n",
    "    results = soup.find(id='__next')\n",
    "    #get href out of arrays\n",
    "    for i in results.find_all('a'):\n",
    "        href.append(i.get('href'))\n",
    "\n",
    "#remove everything that isn't a strain\n",
    "drop = ['/news/cannabis-101/sativa-indica-and-hybrid-differences-between-cannabis-types',\n",
    "     '/strains/lists/category/sativa',\n",
    "     '/strains/lists/category/indica',\n",
    "     '/strains/lists/category/hybrid',\n",
    "     'https://www.leafly.com/',\n",
    "     'https://www.leafly.com/strains']\n",
    "\n",
    "for x in range(1, 115):\n",
    "    drop.append('/strains?sort=name&page=' + str(x))\n",
    "for y in drop:\n",
    "    href.remove(y)\n",
    "href = [e for e in href if e not in drop]\n",
    "\n",
    "def clean_names(x):\n",
    "    '''it is easier to clean the names from the href than it is to scrape them'''\n",
    "    if 'strains' in x:\n",
    "        x = x.replace('strains', ',')\n",
    "    x = x.replace('/', '')\n",
    "    x = x.replace('-', ' ')\n",
    "    x = x.replace(',', ', ')\n",
    "    x = x.title()\n",
    "    if x[0] ==',':\n",
    "        x = x[1:]\n",
    "    if x[0] ==' ':\n",
    "        x = x[1:]\n",
    "    return x\n",
    "\n",
    "#go to each strain page and get the goods\n",
    "#save all our information\n",
    "info = []\n",
    "\n",
    "for g in href:\n",
    "    \n",
    "    #go to the page for the strain\n",
    "    URL = 'https://www.leafly.com'+g\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    #get description\n",
    "    description = []\n",
    "    des = soup.find('div', attrs={'class': \"md:mb-xxl strain__description relative overflow-hidden\"})\n",
    "    if des is not None:\n",
    "        description = des.p\n",
    "        if description is not None:\n",
    "            description = description.text\n",
    "\n",
    "    #get name\n",
    "    name = clean_names(g)\n",
    "\n",
    "    #get terpene info \n",
    "    bar = soup.find('div', attrs={'class': \"flex flex-row flex-grow mt-xs\"})\n",
    "\n",
    "    tech_terp = []\n",
    "    terpene_description = []\n",
    "    terpene_des = []\n",
    "    #some strains have no terpene info, we need to skip through those\n",
    "    if bar is not None:\n",
    "        for i in bar:\n",
    "            #this one was easy, just getting the text out of the div\n",
    "            terpene_description= i.find('div', attrs={'class': 'font-normal'})\n",
    "            terpene_des.append(terpene_description.text)\n",
    "\n",
    "            #this is to get the info out of the style bar, those two contain the real life name and percentage of bar they represent\n",
    "            tech_terp.append(i.attrs['style'])\n",
    "            tech_terp.append(i.attrs['data-testid'])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    #okay so now to get the lineage \n",
    "    line = soup.find('div', attrs={'class': 'font-bold font-headers lineage__container px-lg text-sm'})\n",
    "    \n",
    "    #not every strain has lineage \n",
    "    \n",
    "    #there are no parents, two, or one\n",
    "    parents = []\n",
    "    left_parent = []\n",
    "    right_parent = []\n",
    "    single_parent = []\n",
    "    children = []\n",
    "    left_child = []\n",
    "    right_child = []\n",
    "    only_child = []\n",
    "    left_single_p_child = []\n",
    "    right_single_p_child = []\n",
    "\n",
    "    if line is not None:\n",
    "\n",
    "        left_parent = line.find('div', attrs={'class': 'lineage__left-parent'})\n",
    "        if left_parent is not None:\n",
    "            get_array = left_parent.find('a')\n",
    "            parents = get_array.get('href')\n",
    "\n",
    "        right_parent = line.find('div', attrs={'class': 'lineage__right-parent'})\n",
    "        if right_parent is not None:\n",
    "            get_array = right_parent.find('a')\n",
    "            parents += (get_array.get('href'))\n",
    "\n",
    "        single_parent = line.find('div', attrs={'class': 'lineage__center-parent'})\n",
    "        if single_parent is not None:\n",
    "            get_array = single_parent.find('a')\n",
    "            parents = (get_array.get('href'))\n",
    "\n",
    "        if parents != []:\n",
    "            parents = clean_names(parents)\n",
    "\n",
    "        #most of the strains have two children, but some only have one \n",
    "\n",
    "        left_child = line.find('div', attrs={'class': 'lineage__left-child--two-parents'})\n",
    "        if left_child is not None:\n",
    "            get_array = left_child.find('a')\n",
    "            children = (get_array.get('href'))\n",
    "\n",
    "        right_child = line.find('div', attrs={'class': 'lineage__right-child--two-parents'})\n",
    "        if right_child is not None:\n",
    "            get_array = right_child.find('a')\n",
    "            children += (get_array.get('href'))\n",
    "\n",
    "        #for the only children, single parent, no grandparents households out there\n",
    "        only_child = line.find('div', attrs={'class': 'lineage__center-child--no-parents'})\n",
    "        if only_child is not None:\n",
    "            get_array = only_child.find('a')\n",
    "            children = (get_array.get('href'))\n",
    "\n",
    "        #children of parents with one parent \n",
    "        left_single_p_child = line.find('div', attrs={'class': 'lineage__left-child--single-parent'})\n",
    "        if left_single_p_child is not None:\n",
    "            get_array = left_single_p_child.find('a')\n",
    "            children = (get_array.get('href'))\n",
    "\n",
    "        right_single_p_child = line.find('div', attrs={'class': 'lineage__right-child--single-parent'})\n",
    "        if right_single_p_child is not None:\n",
    "            get_array = right_single_p_child.find('a')\n",
    "            children += (get_array.get('href'))\n",
    "\n",
    "        if children != []:\n",
    "            children = clean_names(children)\n",
    "            \n",
    "    \n",
    "    #get images\n",
    "    pic = []\n",
    "    img = soup.find('meta', attrs={'itemprop': 'image'})\n",
    "    if img is not None:\n",
    "        pic = img.attrs['content']\n",
    "        \n",
    "    #getting thc content \n",
    "    thc_content = []\n",
    "    thc = soup.find('div', attrs={'class':'font-body'})\n",
    "    if thc is not None:\n",
    "        thc_content = thc.text\n",
    "        \n",
    "    #now get rating\n",
    "    rating = []\n",
    "    rate = soup.find('div', attrs={'class':'self-end'})\n",
    "    if rate is not None:\n",
    "        rate = rate.p\n",
    "        if rate is not None:\n",
    "            rating = rate.text\n",
    "    \n",
    "    #get reported feelings via the reviews\n",
    "    #connect to chrome\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    #click the load button until it doesn't exist anymore\n",
    "    #also gets through age gate\n",
    "    #and closes newsletter pop up\n",
    "    #this took so damn long \n",
    "    url = \"https://www.leafly.com\"+g+\"/reviews\"\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//button[@data-testid='age-gate-yes-button']\").click()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if driver is not None:\n",
    "            more_buttons = driver.find_element_by_xpath(\"//button[text()='load more']\")\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    if more_buttons.is_displayed():\n",
    "                        driver.execute_script(\"arguments[0].click();\", more_buttons)\n",
    "                        time.sleep(1)\n",
    "                except StaleElementReferenceException:\n",
    "                    break\n",
    "                try:\n",
    "                    driver.find_element_by_xpath(\"/html/body/div[1]/div[8]/div/button\").click()\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if more_buttons.is_displayed():\n",
    "                        driver.execute_script(\"arguments[0].click();\", more_buttons)\n",
    "                        time.sleep(1)\n",
    "                except StaleElementReferenceException:\n",
    "                    break\n",
    "                \n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    if page_source is not None:\n",
    "\n",
    "        #get the reported feelings for each strain\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        w = soup.find_all('span', attrs={'class' : 'tag mr-sm'})\n",
    "\n",
    "        reported_feelings = []\n",
    "        for i in w:\n",
    "            reported_feelings.append(i.text)\n",
    "\n",
    "        #finally, save it all in a list\n",
    "        info.append([name, description, terpene_des, tech_terp, parents, children, reported_feelings, pic, thc_content, rating])\n",
    "\n",
    "        #so I can save it to a dataframe \n",
    "        df = pd.DataFrame(info, columns=['name', 'description', 'terpene_description', 'technical_terpenes', 'parents',\n",
    "                                        'children', 'reported_feelings', 'pic', 'thc_content', 'rating'])\n",
    "        df.to_csv('leafly_info_cont_peanut_.csv')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "     #   print('Strain name:', name)\n",
    "     #   print('Description:', description)\n",
    "     #   print('Terpene Description:', terpene_des)\n",
    "     #   print('Terpene Details:', tech_terp)\n",
    "     #   print('Parents:', parents)\n",
    "     #   print('Children:', children)\n",
    "     #   print('Reported feelings:', reported_feelings)\n",
    "\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am going just scraping partial info \n",
    "#just reported feelings, images, thc content, and rating\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    " \n",
    "\"\"\"\n",
    "#start by getting the strain names/links\n",
    "URL = 'https://www.leafly.com/strains?sort=name'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "href = []\n",
    "for x in range(1, 115):\n",
    "    #loop through all strain webpages\n",
    "    URL = 'https://www.leafly.com/strains?sort=name&page='+ str(x)\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    #get arrays containing strain info\n",
    "    results = soup.find(id='__next')\n",
    "    #get href out of arrays\n",
    "    for i in results.find_all('a'):\n",
    "        href.append(i.get('href'))\n",
    "\n",
    "#remove everything that isn't a strain\n",
    "drop = ['/news/cannabis-101/sativa-indica-and-hybrid-differences-between-cannabis-types',\n",
    "     '/strains/lists/category/sativa',\n",
    "     '/strains/lists/category/indica',\n",
    "     '/strains/lists/category/hybrid',\n",
    "     'https://www.leafly.com/',\n",
    "     'https://www.leafly.com/strains']\n",
    "\n",
    "for x in range(1, 115):\n",
    "    drop.append('/strains?sort=name&page=' + str(x))\n",
    "for y in drop:\n",
    "    href.remove(y)\n",
    "href = [e for e in href if e not in drop]\n",
    "\n",
    "def clean_names(x):\n",
    "    '''it is easier to clean the names from the href than it is to scrape them'''\n",
    "    if 'strains' in x:\n",
    "        x = x.replace('strains', ',')\n",
    "    x = x.replace('/', '')\n",
    "    x = x.replace('-', ' ')\n",
    "    x = x.replace(',', ', ')\n",
    "    x = x.title()\n",
    "    if x[0] ==',':\n",
    "        x = x[1:]\n",
    "    if x[0] ==' ':\n",
    "        x = x[1:]\n",
    "    return x\n",
    "\"\"\"\n",
    "info = []\n",
    "\n",
    "for g in href:\n",
    "    \n",
    "    #get name\n",
    "    name = clean_names(g)\n",
    "    \n",
    "    #get reported feelings via the reviews\n",
    "    #connect to chrome\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    #options.add_argument('--incognito')\n",
    "    #options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    #click the load button until it doesn't exist anymore\n",
    "    url = \"https://www.leafly.com\"+g+\"/reviews\"\n",
    "    url = \"https://www.leafly.com\"+g+\"/reviews\"\n",
    "    driver.get(url)\n",
    "    driver.find_element_by_xpath(\"//button[@data-testid='age-gate-yes-button']\").click()\n",
    "    try:\n",
    "        if driver is not None:\n",
    "            more_buttons = driver.find_element_by_xpath(\"//button[text()='load more']\")\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    if more_buttons.is_displayed():\n",
    "                        driver.execute_script(\"arguments[0].click();\", more_buttons)\n",
    "                        time.sleep(1)\n",
    "                except StaleElementReferenceException:\n",
    "                    break\n",
    "                try:\n",
    "                    driver.find_element_by_xpath(\"/html/body/div[1]/div[8]/div/button\").click()\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if more_buttons.is_displayed():\n",
    "                        driver.execute_script(\"arguments[0].click();\", more_buttons)\n",
    "                        time.sleep(1)\n",
    "                except StaleElementReferenceException:\n",
    "                    break\n",
    "                \n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    #get the reported feelings for each strain\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    w = soup.find_all('span', attrs={'class' : 'tag mr-sm'})\n",
    "\n",
    "    reported_feelings = []\n",
    "    for i in w:\n",
    "        reported_feelings.append(i.text)\n",
    "\n",
    "    #get images\n",
    "    pic = []\n",
    "    img = soup.find('meta', attrs={'itemprop': 'image'})\n",
    "    if img is not None:\n",
    "        pic = img.attrs['content']\n",
    "        \n",
    "    #getting thc content \n",
    "    thc_content = []\n",
    "    thc = soup.find('div', attrs={'class':'font-body'})\n",
    "    if thc is not None:\n",
    "        thc_content = thc.text\n",
    "        \n",
    "    #now get rating\n",
    "    rating = []\n",
    "    rate = soup.find('div', attrs={'class':'self-end'})\n",
    "    if rate is not None:\n",
    "        rate = rate.p\n",
    "        if rate is not None:\n",
    "            rating = rate.text\n",
    "\n",
    "    #finally, save it all in a list\n",
    "    info.append([name, reported_feelings, pic, thc_content, rating])\n",
    "    \n",
    "    #so I can save it to a dataframe \n",
    "    df = pd.DataFrame(info, columns=['name', 'reported_feelings', 'pic', 'thc_content', 'rating'])\n",
    "    df.to_csv('leafly_feelings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the pop up! finally found it \n",
    "\n",
    "<button class=\"absolute mr-lg mt-lg newsletter__close right-0 top-0\" aria-label=\"Close newsletter signup modal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting images, THC content, and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://public.leafly.com/strains/flowers/2-fast-2-vast-flower.png'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first get images\n",
    "\n",
    "img = soup.find('meta', attrs={'itemprop': 'image'})\n",
    "if img is not None:\n",
    "    pic = img.attrs['content']\n",
    "    \n",
    "pic\n",
    "\n",
    "#remember to clean out pngs when cleaning/merging this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This info is sourced from our readers and is not a substitute for professional medical advice. Seek the advice of a health professional before using cannabis for a medical condition.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting thc content \n",
    "\n",
    "thc = soup.find('div', attrs={'class':'font-body'})\n",
    "if thc is not None:\n",
    "    thc_content = thc.text\n",
    "    \n",
    "thc_content\n",
    "\n",
    "#add to cleaning, removing following text when no thc content\n",
    "#'This info is sourced from our readers and is not a substitute for professional medical advice. \n",
    "#Seek the advice of a health professional before using cannabis for a medical condition.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a review'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now get rating\n",
    "\n",
    "rate = soup.find('div', attrs={'class':'self-end'})\n",
    "if rate is not None:\n",
    "    rate = rate.p\n",
    "    if rate is not None:\n",
    "        rating = rate.text\n",
    "rating\n",
    "        \n",
    "#add to cleaning:\n",
    "#remove 'Write a review' when no rating \n",
    "#take the first 3 characters of the string(contains actual rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting why I got the wrong images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "URL = 'https://www.leafly.com/strains/blue-dream'\n",
    "page = requests.get(URL)\n",
    "time.sleep(5)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li class=\"slider-slide slide-visible\" style=\"box-sizing:border-box;display:inline-block;height:auto;left:0;list-style-type:none;margin-bottom:auto;margin-left:0;margin-right:0;margin-top:auto;-moz-box-sizing:border-box;position:absolute;top:0;transform:scale(1);transition:transform .4s linear;vertical-align:top;width:0\" tabindex=\"-1\"><img alt=\"Picture of Blue Dream strain\" class=\"lazyload m-auto bg-white\" data-src=\"https://leafly-public.imgix.net/web-strain-explorer/1x1-ffffffff.png\" height=\"440\" src=\"https://leafly-public.imgix.net/web-strain-explorer/1x1-ffffffff.png\" width=\"440\"/></li>\n",
      "\n",
      "<li class=\"slider-slide\" style=\"box-sizing:border-box;display:inline-block;height:auto;left:0;list-style-type:none;margin-bottom:auto;margin-left:0;margin-right:0;margin-top:auto;-moz-box-sizing:border-box;position:absolute;top:0;transform:scale(1);transition:transform .4s linear;vertical-align:top;width:0\" tabindex=\"-1\"><img alt=\"Leafly flower of Blue Dream\" class=\"m-auto bg-white\" src=\"https://public.leafly.com/strains/flowers/blue-dream-flower.svg\" style=\"filter:blur(0px)\"/></li>\n",
      "\n",
      "<li class=\"slider-slide\" style=\"box-sizing:border-box;display:inline-block;height:auto;left:0;list-style-type:none;margin-bottom:auto;margin-left:0;margin-right:0;margin-top:auto;-moz-box-sizing:border-box;position:absolute;top:0;transform:scale(1);transition:transform .4s linear;vertical-align:top;width:0\" tabindex=\"-1\"><div class=\"m-auto bg-white flex flex-col items-center justify-center\" style=\"height:300px\"><p class=\"text-deep-green text-sm font-bold pb-sm px-xxl\">Check out photos people have shared with us</p><a class=\"button\" href=\"/strains/blue-dream/photos\" style=\"width:80%\">photos</a></div></li>\n"
     ]
    }
   ],
   "source": [
    "img_cont = soup.findAll('li', attrs={'class':'slider-slide'})\n",
    "print(img_cont[0])\n",
    "print()\n",
    "print(img_cont[1])\n",
    "print()\n",
    "print(img_cont[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('img', attrs={'class': 'm-auto bg-white ls-is-cached lazyloaded'})\n",
    "#this is what I actually need from the website to get the exact image I want\n",
    "#but I need to find a way around their lazyloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what I scrape\n",
    "\n",
    "<img alt=\"Picture of Blue Dream strain\" class=\"lazyload m-auto bg-white\" \n",
    "data-src=\"https://leafly-public.imgix.net/web-strain-explorer/1x1-ffffffff.png\" \n",
    "height=\"440\" src=\"https://leafly-public.imgix.net/web-strain-explorer/1x1-ffffffff.png\" width=\"440\"/></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is actually on the website\n",
    "\n",
    "<img src=\"https://leafly-production.imgix.net/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fleafly-images%2Fflower-images%2Fblue-dream.png?w=440&amp;h=440&amp;auto=format&amp;fit=crop&amp;dpr=2&amp;q=25&amp;ixlib=js-2.3.1&amp;s=4fd778910e03234e4cee02cda5780f2b\"\n",
    "data-src=\"https://leafly-production.imgix.net/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fleafly-images%2Fflower-images%2Fblue-dream.png?w=440&amp;h=440&amp;auto=format&amp;fit=crop&amp;dpr=2&amp;q=25&amp;ixlib=js-2.3.1&amp;s=4fd778910e03234e4cee02cda5780f2b\" \n",
    "alt=\"Picture of Blue Dream strain\" width=\"440\" height=\"440\" class=\"m-auto bg-white ls-is-cached lazyloaded \">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from a page I did not click on at all, hoping to see what is there without triggering\n",
    "#js functions with scroll/click/etc\n",
    "\n",
    "<meta itemprop=\"image\" content=\"https://s3-us-west-2.amazonaws.com/leafly-images/flower-images/blue-dream.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<meta content=\"https://s3-us-west-2.amazonaws.com/leafly-images/flower-images/blue-dream.png\" itemprop=\"image\"/>]\n"
     ]
    }
   ],
   "source": [
    "take_2 = soup.findAll('meta', attrs={'itemprop':\"image\"})\n",
    "print(take_2)\n",
    "\n",
    "#awesome, this is bringing me a real image\n",
    "#time to test with more strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing new img method with just 5 out of 115 web pages worth of strains\n",
    "#it went well, not many strains have pictures but the ones that do are showing up!!\n",
    "#starting with complete strain list at 6:32pm, 6/4/20\n",
    "\n",
    "URL = 'https://www.leafly.com/strains?sort=name'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "href = []\n",
    "for x in range(1, 115):\n",
    "    #loop through all strain webpages\n",
    "    URL = 'https://www.leafly.com/strains?sort=name&page='+ str(x)\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    #get arrays containing strain info\n",
    "    results = soup.find(id='__next')\n",
    "    #get href out of arrays\n",
    "    for i in results.find_all('a'):\n",
    "        href.append(i.get('href'))\n",
    "\n",
    "#remove everything that isn't a strain\n",
    "drop = ['/news/cannabis-101/sativa-indica-and-hybrid-differences-between-cannabis-types',\n",
    "     '/strains/lists/category/sativa',\n",
    "     '/strains/lists/category/indica',\n",
    "     '/strains/lists/category/hybrid',\n",
    "     'https://www.leafly.com/',\n",
    "     'https://www.leafly.com/strains']\n",
    "\n",
    "for x in range(1, 115):\n",
    "    drop.append('/strains?sort=name&page=' + str(x))\n",
    "for y in drop:\n",
    "    href.remove(y)\n",
    "href = [e for e in href if e not in drop]\n",
    "\n",
    "def clean_names(x):\n",
    "    '''it is easier to clean the names from the href than it is to scrape them'''\n",
    "    if 'strains' in x:\n",
    "        x = x.replace('strains', ',')\n",
    "    x = x.replace('/', '')\n",
    "    x = x.replace('-', ' ')\n",
    "    x = x.replace(',', ', ')\n",
    "    x = x.title()\n",
    "    if x[0] ==',':\n",
    "        x = x[1:]\n",
    "    if x[0] ==' ':\n",
    "        x = x[1:]\n",
    "    return x\n",
    "\n",
    "info = []\n",
    "\n",
    "for g in href:\n",
    "    \n",
    "    #go to the page for the strain\n",
    "    URL = 'https://www.leafly.com'+g\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    #get name\n",
    "    name = clean_names(g)\n",
    "    \n",
    "    picture = soup.findAll('meta', attrs={'itemprop':\"image\"})\n",
    "    \n",
    "    #finally, save it all in a list\n",
    "    info.append([name, picture])\n",
    "\n",
    "    #so I can save it to a dataframe \n",
    "    df = pd.DataFrame(info, columns=['name', 'picture'])\n",
    "    df.to_csv('just_pictures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting around the load more button for reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://www.leafly.com/strains/100-og/reviews\"\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "button = soup.findAll('button', attrs={'class': \"button\"})\n",
    "button\n",
    "\n",
    "for i in button:\n",
    "    if i.text == 'load more':\n",
    "        the_button = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<button class=\"button\" data-testid=\"age-gate-no-button\" id=\"tou-cancel\" tabindex=\"0\">no</button>,\n",
       " <button class=\"button ml-md\" data-testid=\"age-gate-yes-button\" id=\"tou-continue\" tabindex=\"0\">yes</button>,\n",
       " <button class=\"button mx-auto\">sign up</button>,\n",
       " <button class=\"button\">load more</button>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"a084c09a6f3f9b3e52ddd513a1ce3fb2\", element=\"a9c5a015-0f00-44ea-92fc-1cec95be9be4\")>\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "url = \"https://www.leafly.com/strains/100-og/reviews\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "#try:\n",
    "button = driver.find_element_by_xpath(\"//button[@data-testid='age-gate-yes-button']\")\n",
    "#cross = driver.find_element_by_id(\"nvpush_cross\")\n",
    "print(button)\n",
    "#button.click()\n",
    "#except:\n",
    "   # pass\n",
    "\n",
    "#while True:\n",
    "#    try:\n",
    "#        loadmore = driver.find_element_by_xpath(\"//button/[text()='load more']/parent::div[@class='w-full flex items-center justify-center lg:justify-start']\")\n",
    "#        loadmore.click()\n",
    "#    except NoSuchElementException:\n",
    "#        print(\"Reached bottom of page\")\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "#connect to chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "#options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "#click the load button until it doesn't exist anymore\n",
    "driver.get(\"https://www.leafly.com/strains/100-og/reviews\")\n",
    "more_buttons = driver.find_element_by_xpath(\"//button[text()='load more']\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        if more_buttons.is_displayed():\n",
    "            driver.execute_script(\"arguments[0].click();\", more_buttons)\n",
    "            time.sleep(1)\n",
    "    except StaleElementReferenceException:\n",
    "        print('maybe no more pages')\n",
    "        break\n",
    "    \n",
    "page_source = driver.page_source\n",
    "\n",
    "#get the reported feelings for each strain\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "w = soup.find_all('span', attrs={'class' : 'tag mr-sm'})\n",
    "\n",
    "reported_feelings = []\n",
    "for i in w:\n",
    "    reported_feelings.append(i.text)\n",
    "    \n",
    "reported_feelings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
